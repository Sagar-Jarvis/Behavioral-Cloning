{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files Submitted & Code Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode.\n",
    "My project includes the following files:\n",
    "\n",
    "1) model.py containing the script to create and train the model.\n",
    "\n",
    "2) drive.py for driving the car in autonomous mode.\n",
    "\n",
    "3) model21.h5 containing a trained convolution neural network.\n",
    "\n",
    "4) Write_up.ipynb for summarizing the results.\n",
    "\n",
    "5) Data_exploration.ipynb (The websocket use to run into errors for training large data on EC2 through chrome and so the cell having the training is incomplete due to that reason)\n",
    "\n",
    "6) runl.mp4\n",
    "\n",
    "7) runl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Submission includes functional code Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python drive.py model21.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. Submission code is usable and readable.\n",
    "\n",
    "The clone3.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy.\n",
    "\n",
    "I wanted to do this project in a lean manner and avoid data generators with lessor parameters and smaller network. I searched, read and talked to my ex-mentor about it. And I came accross many awesome work done by people and was amazed at what others have done. I came accross 32x32 of sizing the images, 16X32, 48X48 and with smaller networks. So I decided to go even smaller resizing of the images 16x16 to try how the network does for only track 1 and adopted a convolutional network with kernel size of (3,3) with 'elu' activation and 'valid' padding. I used Max pooling with the kernel size of (4, 4) and then used a dropout and flattened to be followed by Dense layer for outputting a single output i.e. Steering angle.\n",
    "\n",
    "Now I used an elu layer instead of RELU it's because over the time experimenting with the network I realized in going lean, the network can be influenced with even a small change. For example I trained it with RELU activation and the newtwork did complete the track 1 but had a little more wobble which made the car near the dirt road get confused and it skirt around the lane markings at the hard left near the dirt road. Now I realized that having smaller and lean network means learning quickly and also take care of not over fitting. Exponential Linear Units are quite famous for making the network learn faster and not saturate giving it a push to mean activation of neurons close to zero. As the network isn't deep enough and with even smaller size of the images for the input I decided on elu.\n",
    "\n",
    "Later for dropout I tried 0.5 and 0.75 just for fun(0.75) as expected the network learned slower and underfitted heavily resulting in slower turns. So I settled for 0.25 itself. That did have little wobble but then I decided to decrease the speed with lesser throtle so that I can have a visibly stable car moving. So going with a very leaner network with 16X16 resized images and only Udacity data and no extra driving data I compromised on the speed to reduce heavily on the wobble. My next target would be to complete the track with full throttle. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________\n",
    "Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "====================================================================================================\n",
    "lambda_1 (Lambda)                (None, 16, 16, 1)     0           lambda_input_1[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_1 (Convolution2D)  (None, 14, 14, 2)     20          lambda_1[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling2d_1 (MaxPooling2D)    (None, 3, 3, 2)       0           convolution2d_1[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "dropout_1 (Dropout)              (None, 3, 3, 2)       0           maxpooling2d_1[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "flatten_1 (Flatten)              (None, 18)            0           dropout_1[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "dense_1 (Dense)                  (None, 1)             19          flatten_1[0][0]                  \n",
    "====================================================================================================\n",
    "Total params: 39\n",
    "Trainable params: 39\n",
    "Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the Udacity data. Controlling the car with the keyboard was quite messy and the data set I created made the car wobble a lot and crash. I explored with converting the images to different color spaces and check out how the network performed. The Gray did ease things but the dirt road was indistinguishable and made the car move into the dirt road. I tried with HLS and HSV as from experience of previous projects these color spaces usually higlight the road differences. I tried using V channel but it still did crash at the first hard left at the dirt road but on the straight roads the model performed better than H channel and equally good as on S channel. I can improve the V channel model by brightness manipulation and increasing all the values across all the images but that would have taken a little more time to process so I worked with S channel and surprisingly it worked fine at all turns and roads with elu activision layer. The data exploration is shown below.\n",
    "\n",
    "I used the left, centre and right camera images and appended it to a list with it's respective steering angles to another list and created a dictionary of these two lists. I also used a little correction factor for left and right images as discussed in the course. For augmenting the data I only used flipping. By using np.fliplr() the network behaved little different than when I used images_train[:,:,::-1]. As both are same I still am not why both methods being same resulting in different driving behaviour. images_train[:,:,::-1] worked fine and after using np.fliplr() there seems to be a little residue left in the weights as I returned to images_train[:,:,::-1] method it required few number of runs of training to get to the good performance but this is my guess. Still I have to explore in this direction.\n",
    "\n",
    "I did not need the use of Data_generators as the memory could handle the training due to the re-sized images of 16X16. I had to decrease the throtle to 0.1 so that I can supress the wobbling but my next try is to have full throtle and finish the track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation sets are created by using train_test_split just after shuffling the dataset. Maxpooling and Dropout are used to avoid overfitting as explained in the above cells. Maxpooling kernel size is (4, 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Adam optimizer so I did have to fine tune the parameters myself. I would like to underline the method in which the Adam optimiers work.\n",
    "\n",
    "1) Compute the gradient and its element-wise square using the current parameters.\n",
    "\n",
    "2) Update the exponential moving average of the 1st-order moment and the 2nd-order moment.\n",
    "\n",
    "3) Compute an unbiased average of the 1st-order moment and 2nd-order moment.\n",
    "\n",
    "4) Compute weight update: 1st-order moment unbiased average divided by the square root of 2nd-order moment unbiased average (and scale by learning rate).\n",
    "\n",
    "5) Apply update to the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function I used the standard Mean Squared Error(MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model summary shows how the model works on 39 parameters and could achieve to finish the track 1 with 9 Epochs. It actually works nicely in 8 Epochs also but I chose to do 9 as little overfit saves my car at the hard right turn to avoid the lane marking(it just touched slightly at 8 Epochs). Now the model did not work well with track 2 as the model isn't working on more data which could generalize for track 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of my exploration and experimentation with the dataset, network and methods from Data_exploration.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### S Channel\n",
    "![alt text](output_images/1.png \"S Channel\")\n",
    "##### V Channel\n",
    "![alt text](output_images/2.png \"V Channel\")\n",
    "##### H Channel\n",
    "![alt text](output_images/4.png \"H Channel\")\n",
    "##### Gray Channel\n",
    "![alt text](output_images/3.png \"Gray Channel\")\n",
    "##### Flipping\n",
    "![alt text](output_images/6.png \"Flipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My output video of the Track 1\n",
    "\n",
    "<video controls src=\"runl.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "This network isn't perfect and it won't help in the real life scenario but I wanted to explore a simple route to finish at least the Track 1. This experiment taught me a mantra, to think first how to solve a problem in a simplest way possible and then build upon that. Even though this model won't work fine on Track 2 untill major tweaks and extra data is added but because I went this route of tackling this project I learned more invaluable things. I got to know more how I can use network building from ground and foundational level to top. I thank those awesome people who have acheived finishing both the tracks with leanest network possible. Now my goal is to finish Track 2 and then work on a YOLO implementation and then my real life video and try coming up with a model in YOLO itself as Deep Learning would be used in all the cars eventually to detect cars and YOLO is the best out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://github.com/nikidimi/carnd-behavioral-cloning\n",
    "- https://medium.com/@xslittlegrass/self-driving-car-in-a-simulator-with-a-tiny-neural-network-13d33b871234\n",
    "- https://github.com/parambharat/CarND-Behavioral-Cloning-P3\n",
    "- Udacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
